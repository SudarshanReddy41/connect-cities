{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNpjRhpsi/HPhs2iUGpWjYg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SudarshanReddy41/initial-agent-service/blob/master/RCD_Construction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvD5hfceKo1c"
      },
      "outputs": [],
      "source": [
        "import requests, os, csv, logging, datetime, time, shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# -------------------\n",
        "# CONFIGURATION\n",
        "# -------------------\n",
        "API_KEY = \"X3gXLEGe4J8qUbkaFpsl2BNUT9R2PmBSJgJ4WX51\"\n",
        "API_ENDPOINT = \"https://api.regulations.gov/v4/documents\"\n",
        "HEADERS = {\"X-Api-Key\": API_KEY}\n",
        "METADATA_CSV = \"construction_metadata.csv\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your Google Drive folder path\n",
        "GDRIVE_FOLDER_PATH = '/content/drive/MyDrive/construction_documents'\n",
        "os.makedirs(GDRIVE_FOLDER_PATH, exist_ok=True)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# -------------------\n",
        "# Search Documents\n",
        "# -------------------\n",
        "def search_documents(term, page_size=100, max_pages=10):\n",
        "    all_docs = []\n",
        "    start_date = datetime.date(2024, 1, 1)\n",
        "    end_date = datetime.date(2025, 4, 4)\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        params = {\n",
        "            'filter[searchTerm]': term,\n",
        "            'filter[documentType]': 'Rule,Proposed Rule',\n",
        "            'filter[postedDate][ge]': start_date.isoformat(),\n",
        "            'filter[postedDate][le]': end_date.isoformat(),\n",
        "            'page[size]': page_size,\n",
        "            'page[number]': page\n",
        "        }\n",
        "        logger.info(f\"Fetching page {page}...\")\n",
        "        response = requests.get(API_ENDPOINT, headers=HEADERS, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            logger.error(f\"API error {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get(\"data\", [])\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        all_docs.extend(data)\n",
        "    return all_docs\n",
        "\n",
        "# -------------------\n",
        "# Download PDF and upload to Google Drive folder\n",
        "# -------------------\n",
        "def download_and_upload_pdf(doc_id, folder_path, retries=3, delay=5):\n",
        "    url = f\"https://downloads.regulations.gov/{doc_id}/content.pdf\"\n",
        "    local_path = f\"{doc_id}.pdf\"\n",
        "    drive_path = os.path.join(folder_path, f\"{doc_id}.pdf\")\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                with open(local_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                shutil.move(local_path, drive_path)\n",
        "                return f\"Uploaded to: {drive_path}\", 200\n",
        "            else:\n",
        "                logger.warning(f\"Failed to download {doc_id}: HTTP {response.status_code}\")\n",
        "                return None, response.status_code\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            time.sleep(delay)\n",
        "    return None, None\n",
        "\n",
        "# -------------------\n",
        "# Save Metadata to CSV\n",
        "# -------------------\n",
        "def save_metadata_to_csv(records, path=METADATA_CSV):\n",
        "    headers = [\"doc_id\", \"document_type\", \"title\", \"posted_date\", \"gdrive_url\", \"download_status\"]\n",
        "    write_header = not os.path.exists(path)\n",
        "\n",
        "    with open(path, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=headers)\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerows(records)\n",
        "\n",
        "# -------------------\n",
        "# Main Pipeline\n",
        "# -------------------\n",
        "def run_pipeline(term='construction safety compliance', max_pages=10):\n",
        "    docs = search_documents(term, page_size=100, max_pages=max_pages)\n",
        "    results = []\n",
        "\n",
        "    for doc in docs:\n",
        "        doc_id = doc.get('id')\n",
        "        attr = doc.get('attributes', {})\n",
        "        gdrive_path, status = download_and_upload_pdf(doc_id, GDRIVE_FOLDER_PATH)\n",
        "\n",
        "        results.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"document_type\": attr.get('documentType', ''),\n",
        "            \"title\": attr.get('title', ''),\n",
        "            \"posted_date\": attr.get('postedDate', ''),\n",
        "            \"gdrive_url\": gdrive_path or '',\n",
        "            \"download_status\": status or ''\n",
        "        })\n",
        "\n",
        "    save_metadata_to_csv(results)\n",
        "    logger.info(f\"Saved metadata for {len(results)} documents.\")\n",
        "\n",
        "# -------------------\n",
        "# Run\n",
        "# -------------------\n",
        "run_pipeline(term='construction safety compliance', max_pages=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymupdf sentence-transformers faiss-cpu\n",
        "!pip install huggingface_hub[hf_xet]\n",
        "!pip install tools"
      ],
      "metadata": {
        "id": "6oCK5wDARiUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pymupdf\n"
      ],
      "metadata": {
        "id": "-_dxFnoHbdX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9b68f8-d71f-4f54-e8d4-4c3d26a20613"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Using cached pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Using cached pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "Installing collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # from PyMuPDF\n",
        "doc = fitz.open  # just check that the function exists\n",
        "print(\"âœ… fitz.open is accessible\")"
      ],
      "metadata": {
        "id": "CccexzMwZDV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import csv\n",
        "import fitz  # PyMuPDF\n",
        "import faiss\n",
        "import numpy as np\n",
        "import textwrap\n",
        "import subprocess\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "# ------------------------\n",
        "# Mount Google Drive\n",
        "# ------------------------\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ------------------------\n",
        "# Configuration\n",
        "# ------------------------\n",
        "PDF_FOLDER = '/content/drive/MyDrive/construction_documents'\n",
        "METADATA_CSV = '/content/drive/MyDrive/construction_metadata_log.csv'\n",
        "EMBED_MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
        "USER_QUERY = \"What are the cybersecurity compliance requirements in these documents?\"\n",
        "\n",
        "# ------------------------\n",
        "# 1. PDF Chunking\n",
        "# ------------------------\n",
        "def extract_text_chunks_from_folder(folder_path, chunk_size=500):\n",
        "    all_chunks = []\n",
        "    pdf_files = [f for f in os.listdir(folder_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "    for pdf_file in pdf_files:\n",
        "        pdf_path = os.path.join(folder_path, pdf_file)\n",
        "        doc = fitz.open(pdf_path)\n",
        "\n",
        "        for page_num, page in enumerate(doc):\n",
        "            text = page.get_text()\n",
        "            wrapped = textwrap.wrap(text, chunk_size)\n",
        "            for i, chunk in enumerate(wrapped):\n",
        "                all_chunks.append({\n",
        "                    \"doc_id\": pdf_file,\n",
        "                    \"chunk_id\": f\"{pdf_file}_p{page_num}_c{i}\",\n",
        "                    \"text\": chunk\n",
        "                })\n",
        "    return all_chunks\n",
        "\n",
        "# ------------------------\n",
        "# 2. Embedding\n",
        "# ------------------------\n",
        "def embed_chunks(chunks, model_name=EMBED_MODEL_NAME, batch_size=32):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [c[\"text\"] for c in chunks]\n",
        "    embeddings = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i + batch_size]\n",
        "        emb = model.encode(batch, convert_to_tensor=False, show_progress_bar=True)\n",
        "        embeddings.extend(emb)\n",
        "\n",
        "    return np.array(embeddings), chunks, model\n",
        "\n",
        "\n",
        "# ------------------------\n",
        "# 3. FAISS Indexing\n",
        "# ------------------------\n",
        "def create_faiss_index(embeddings):\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "    return index\n",
        "\n",
        "# ------------------------\n",
        "# 4. Retrieval\n",
        "# ------------------------\n",
        "def retrieve_relevant_chunks(query, index, chunks, embed_model, k=5):\n",
        "    query_vec = embed_model.encode([query])\n",
        "    D, I = index.search(np.array(query_vec), k)\n",
        "    retrieved = set(I[0])\n",
        "    return [chunks[i] for i in I[0]], retrieved\n",
        "\n",
        "# ------------------------\n",
        "# 5. Query LLaMA 4 (via Ollama CLI)\n",
        "# ------------------------\n",
        "def query_llama4(context_chunks, user_query):\n",
        "    context_text = \"\\n\\n\".join([f\"[{c['chunk_id']}] {c['text']}\" for c in context_chunks])\n",
        "    prompt = f\"\"\"You are a compliance analyst. Given the following regulatory document sections, answer the question below with precision.\n",
        "\n",
        "### Context:\n",
        "{context_text}\n",
        "\n",
        "### Question:\n",
        "{user_query}\n",
        "\n",
        "### Answer:\"\"\"\n",
        "\n",
        "    result = subprocess.run(\n",
        "        ['ollama', 'run', 'llama4'],\n",
        "        input=prompt.encode('utf-8'),\n",
        "        stdout=subprocess.PIPE\n",
        "    )\n",
        "    return result.stdout.decode()\n",
        "\n",
        "# ------------------------\n",
        "# 6. Metadata Logging\n",
        "# ------------------------\n",
        "def save_metadata_to_csv(chunks, retrieved_ids, csv_path=METADATA_CSV):\n",
        "    fieldnames = [\"doc_id\", \"chunk_id\", \"text_length\", \"retrieved_for_answer\"]\n",
        "    write_header = not os.path.exists(csv_path)\n",
        "\n",
        "    with open(csv_path, \"a\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            writer.writerow({\n",
        "                \"doc_id\": chunk[\"doc_id\"],\n",
        "                \"chunk_id\": chunk[\"chunk_id\"],\n",
        "                \"text_length\": len(chunk[\"text\"]),\n",
        "                \"retrieved_for_answer\": \"YES\" if i in retrieved_ids else \"NO\"\n",
        "            })\n",
        "\n",
        "# ------------------------\n",
        "# 7. Main Pipeline\n",
        "# ------------------------\n",
        "def main():\n",
        "    if not os.path.exists(PDF_FOLDER):\n",
        "        raise FileNotFoundError(f\"Folder not found: {PDF_FOLDER}\")\n",
        "\n",
        "    print(\"Extracting and chunking PDFs...\")\n",
        "    chunks = extract_text_chunks_from_folder(PDF_FOLDER)\n",
        "\n",
        "    print(\"Embedding chunks...\")\n",
        "    embeddings, chunks, embed_model = embed_chunks(chunks)\n",
        "\n",
        "    print(\"Creating FAISS index...\")\n",
        "    index = create_faiss_index(embeddings)\n",
        "\n",
        "    print(\"Retrieving relevant chunks for query...\")\n",
        "    top_chunks, retrieved_ids = retrieve_relevant_chunks(USER_QUERY, index, chunks, embed_model)\n",
        "\n",
        "    print(\"Saving metadata...\")\n",
        "    save_metadata_to_csv(chunks, retrieved_ids)\n",
        "\n",
        "    print(\"Querying LLaMA 4...\")\n",
        "    response = query_llama4(top_chunks, USER_QUERY)\n",
        "\n",
        "    print(\"\\n--- LLaMA 4 Response ---\\n\")\n",
        "    print(response)\n",
        "\n",
        "# ------------------------\n",
        "# Run\n",
        "# ------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ijbuNZP1PMHa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}