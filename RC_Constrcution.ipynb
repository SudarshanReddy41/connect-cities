{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMs5f9m4D9ZzL2NgZbBrrca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SudarshanReddy41/initial-agent-service/blob/master/RC_Constrcution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6p1U1HizMLdW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from urllib.parse import urlparse, urlunparse\n",
        "\n",
        "# Define the base URL and API key\n",
        "API_KEY = \"X3gXLEGe4J8qUbkaFpsl2BNUT9R2PmBSJgJ4WX51\"\n",
        "API_ENDPOINT = \"https://api.regulations.gov/v4/documents\"\n",
        "file_url = \"https://downloads.regulations.gov/\"\n",
        "HEADERS = {\"X-Api-Key\": API_KEY,\n",
        "           \"Accept\" : \"application/json\"}\n",
        "DOWNLOAD_DIR = \"rule_documents\"\n",
        "PAGE_SIZE = 100000\n",
        "\n",
        "\n",
        "def download_pdf(document_id, file_url):\n",
        "    print(\"üì• Starting PDF download...\")\n",
        "    file_url = file_url + document_id + \"/content.pdf\"\n",
        "    # Create directory for the document\n",
        "    document_dir = os.path.join(DOWNLOAD_DIR, document_id)\n",
        "    os.makedirs(document_dir, exist_ok=True)\n",
        "    print(f\"üìÅ Document directory: {document_dir}\")\n",
        "\n",
        "    # Use the filename from the URL\n",
        "    filename = os.path.basename(file_url)\n",
        "    save_path = os.path.join(document_dir, filename)\n",
        "\n",
        "    print(f\"‚û°Ô∏è  Downloading: {file_url}\")\n",
        "    print(f\"üìÅ Saving to: {save_path}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(file_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with open(save_path, \"wb\") as f:\n",
        "            for chunk in response.iter_content(1024):\n",
        "                f.write(chunk)\n",
        "        print(f\"‚úÖ Successfully downloaded: {filename}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to download {document_id}: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def search_documents(search_term):\n",
        "    page = 1\n",
        "    total_pages = 1  # Initialize to enter the loop\n",
        "\n",
        "    while page <= total_pages:\n",
        "        params = {\n",
        "            'filter[searchTerm]': search_term,\n",
        "            'page[number]': page,\n",
        "            'page[size]': PAGE_SIZE\n",
        "        }\n",
        "\n",
        "        print(f\"\\nüìÑ Fetching page {page}...\")\n",
        "        response = requests.get(API_ENDPOINT, params=params, headers=HEADERS)\n",
        "        print(f\"‚û°Ô∏è  Response : {response}\")\n",
        "        if response.status_code != 200:\n",
        "            print(f\"‚ùå API error: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            data = response.json()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to parse JSON: {e}\")\n",
        "            print(f\"Raw response:\\n{response.text}\")\n",
        "            break\n",
        "        documents = data.get(\"data\", [])\n",
        "\n",
        "        meta = data.get(\"meta\", {})\n",
        "        total_pages = meta.get(\"totalPages\", 1)\n",
        "\n",
        "        print(f\"üîç Found {len(documents)} documents on page {page} / {total_pages}\")\n",
        "\n",
        "        for doc in documents:\n",
        "            doc_attrs = doc.get(\"attributes\", {})\n",
        "            document_id = doc.get(\"id\")\n",
        "            document_type = doc_attrs.get(\"documentType\")\n",
        "            title = doc_attrs.get(\"title\")\n",
        "            if \"Rule\" in document_type:\n",
        "              print(f\"Rule: {title}\")\n",
        "              download_pdf(document_id, file_url)\n",
        "\n",
        "        page += 1\n",
        "        time.sleep(0.5)  # Be kind to the API server!\n",
        "\n",
        "# === Run the function with your search term\n",
        "search_documents(\"construction safety compliance\")\n"
      ],
      "metadata": {
        "id": "O9wJyjlFMSBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests, os, csv, logging, datetime, time, shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# -------------------\n",
        "# CONFIGURATION\n",
        "# -------------------\n",
        "API_KEY = \"X3gXLEGe4J8qUbkaFpsl2BNUT9R2PmBSJgJ4WX51\"\n",
        "API_ENDPOINT = \"https://api.regulations.gov/v4/documents\"\n",
        "HEADERS = {\"X-Api-Key\": API_KEY}\n",
        "METADATA_CSV = \"insurance_metadata.csv\"\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define your Google Drive folder path\n",
        "GDRIVE_FOLDER_PATH = '/content/drive/MyDrive/insurance_documents'\n",
        "os.makedirs(GDRIVE_FOLDER_PATH, exist_ok=True)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger()\n",
        "\n",
        "# -------------------\n",
        "# Search Documents\n",
        "# -------------------\n",
        "def search_documents(term, page_size=100, max_pages=10):\n",
        "    all_docs = []\n",
        "    start_date = datetime.date(2024, 1, 1)\n",
        "    end_date = datetime.date(2025, 1, 1)\n",
        "\n",
        "    for page in range(1, max_pages + 1):\n",
        "        params = {\n",
        "            'filter[searchTerm]': term,\n",
        "            'filter[documentType]': 'Rule,Proposed Rule',\n",
        "            'filter[postedDate][ge]': start_date.isoformat(),\n",
        "            'filter[postedDate][le]': end_date.isoformat(),\n",
        "            'page[size]': page_size,\n",
        "            'page[number]': page\n",
        "        }\n",
        "        logger.info(f\"Fetching page {page}...\")\n",
        "        response = requests.get(API_ENDPOINT, headers=HEADERS, params=params)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            logger.error(f\"API error {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        data = response.json().get(\"data\", [])\n",
        "        if not data:\n",
        "            break\n",
        "\n",
        "        all_docs.extend(data)\n",
        "    return all_docs\n",
        "\n",
        "# -------------------\n",
        "# Download PDF and upload to Google Drive folder\n",
        "# -------------------\n",
        "def download_and_upload_pdf(doc_id, folder_path, retries=3, delay=5):\n",
        "    url = f\"https://downloads.regulations.gov/{doc_id}/content.pdf\"\n",
        "    local_path = f\"{doc_id}.pdf\"\n",
        "    drive_path = os.path.join(folder_path, f\"{doc_id}.pdf\")\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS, timeout=15)\n",
        "            if response.status_code == 200:\n",
        "                with open(local_path, 'wb') as f:\n",
        "                    f.write(response.content)\n",
        "\n",
        "                shutil.move(local_path, drive_path)\n",
        "                return f\"Uploaded to: {drive_path}\", 200\n",
        "            else:\n",
        "                logger.warning(f\"Failed to download {doc_id}: HTTP {response.status_code}\")\n",
        "                return None, response.status_code\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Attempt {attempt+1} failed: {e}\")\n",
        "            time.sleep(delay)\n",
        "    return None, None\n",
        "\n",
        "# -------------------\n",
        "# Save Metadata to CSV\n",
        "# -------------------\n",
        "def save_metadata_to_csv(records, path=METADATA_CSV):\n",
        "    headers = [\"doc_id\", \"document_type\", \"title\", \"posted_date\", \"gdrive_url\", \"download_status\"]\n",
        "    write_header = not os.path.exists(path)\n",
        "\n",
        "    with open(path, 'a', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=headers)\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerows(records)\n",
        "\n",
        "# -------------------\n",
        "# Main Pipeline\n",
        "# -------------------\n",
        "def run_pipeline(term='insurance', max_pages=10):\n",
        "    docs = search_documents(term, page_size=100, max_pages=max_pages)\n",
        "    results = []\n",
        "\n",
        "    for doc in docs:\n",
        "        doc_id = doc.get('id')\n",
        "        attr = doc.get('attributes', {})\n",
        "        gdrive_path, status = download_and_upload_pdf(doc_id, GDRIVE_FOLDER_PATH)\n",
        "\n",
        "        results.append({\n",
        "            \"doc_id\": doc_id,\n",
        "            \"document_type\": attr.get('documentType', ''),\n",
        "            \"title\": attr.get('title', ''),\n",
        "            \"posted_date\": attr.get('postedDate', ''),\n",
        "            \"gdrive_url\": gdrive_path or '',\n",
        "            \"download_status\": status or ''\n",
        "        })\n",
        "\n",
        "    save_metadata_to_csv(results)\n",
        "    logger.info(f\"Saved metadata for {len(results)} documents.\")\n",
        "\n",
        "# -------------------\n",
        "# Run\n",
        "# -------------------\n",
        "run_pipeline(term='insurance', max_pages=10)\n"
      ],
      "metadata": {
        "id": "l0NFiEhlUpgw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5eb3f5f4-0e92-489b-a6cb-b19d8bf2eac9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Failed to download NCUA-2024-0038-0001: HTTP 403\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive2.auth import GoogleAuth\n",
        "from pydrive2.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "import os\n",
        "\n",
        "# Step 1: Authenticate with Google\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Step 2: Setup GoogleAuth for Colab\n",
        "gauth = GoogleAuth()\n",
        "\n",
        "# Force manual flow if automatic fails\n",
        "gauth.LoadCredentialsFile(\"credentials.json\")\n",
        "\n",
        "if gauth.credentials is None:\n",
        "    # First-time login, use interactive browser\n",
        "    gauth.LocalWebserverAuth()\n",
        "elif gauth.access_token_expired:\n",
        "    # Refresh expired credentials\n",
        "    gauth.Refresh()\n",
        "else:\n",
        "    # Authorize in-memory if credentials already exist\n",
        "    gauth.Authorize()\n",
        "\n",
        "# Save credentials for future use\n",
        "gauth.SaveCredentialsFile(\"credentials.json\")\n",
        "\n",
        "# Step 3: Create Google Drive object\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "metadata": {
        "id": "bO_6wOMS8Dua"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}